#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun 20 16:47:35 2018
"""

import os
import time
import h5py
import math
import pickle
import numpy as np
import pandas as pd
import cv2
import threading
import queue
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import misc, ndimage
from sklearn import model_selection, preprocessing, metrics
from sklearn.utils import shuffle
from skimage import transform
from tqdm import tqdm
from keras.regularizers import l2
from keras.models import Model, load_model
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from keras import backend as K
from keras.losses import binary_crossentropy
import keras.backend.tensorflow_backend as KTF
import tensorflow as tf
from tensorflow.python.client import device_lib
from os import listdir
from os.path import isfile, join
from scipy.ndimage.filters import gaussian_filter
from skimage.color import rgb2grey, grey2rgb
from scipy.ndimage.interpolation import map_coordinates
from skimage.util import random_noise
from keras.utils import Sequence
from keras import backend as K
import random

DATA_PATH =os.getcwd()+ '/Data/'

TRAIN_PATH = os.path.join(DATA_PATH, 'train')
TRAIN_MASKS_PATH = os.path.join(DATA_PATH, 'train_masks')
ASSETS_PATH = os.path.join(DATA_PATH, 'assets')
MODELS_PATH = os.path.join(ASSETS_PATH, 'models')
TENSORBOARD_PATH = os.path.join(ASSETS_PATH, 'tensorboard')


# Constants
HEIGHT_ORIG = 2672
WIDTH_ORIG = 2688
CHANNELS_ORIG = 3

HEIGHT = 640
WIDTH = 640
CHANNELS = 1
new_shape = (HEIGHT, WIDTH, CHANNELS)
mask_shape = (new_shape[0], new_shape[1], 1)


BATCH_SIZE = 2

def get_img_id(img_path):
    return img_path[:15]

#img_ids = list(map(get_img_id, list(train_masks_df.img.values)))

def load_image_disk(img_id, folder=TRAIN_PATH):
    img = misc.imread(os.path.join(folder, "image_"+img_id + ".tif"))
    
    if len(img.shape) == 2:
        img = np.expand_dims(img, axis=3)
    return img

def get_image(img_id):
    return train_imgs[img_id]

# Return mask as 1/0 binary img with single channel
def load_mask_disk(img_id, folder=TRAIN_MASKS_PATH, filetype='tif'):
    mask = misc.imread(os.path.join(folder,  "mask_{}.{}".format(img_id, filetype)), flatten=True)
    mask[mask > 128] = 1
    if len(mask.shape) == 2:
        mask = mask.reshape(mask.shape[0], mask.shape[1])
    return mask

def get_mask(img_id):
    return train_masks[img_id]

def gray2rgb(img):
    img = np.squeeze(img)
    w, h = img.shape
    ret = np.empty((w, h, 3), dtype=np.uint8)
    ret[:, :, 0] = img
    ret[:, :, 1] = img
    ret[:, :, 2] = img
    return ret

def resize_img(img, new_s = new_shape):
    return transform.resize(img, new_s)

def plot_image(img_id):
    img = misc.imread(os.path.join(TRAIN_PATH, "image_"+img_id + ".tif"))
    imgplot = plt.imshow(img)
    plt.axis('off')
    plt.show()



random.seed(42)
onlyfiles = [f for f in listdir(TRAIN_PATH) if isfile(join(TRAIN_PATH, f))]
imageid=['_'.join(file.split('.')[0].split('_')[1:]) for file in onlyfiles]



dummy_ids, test_ids = model_selection.train_test_split(imageid, random_state=21, test_size=0.03)
train_ids, validation_ids = model_selection.train_test_split(dummy_ids, random_state=21, test_size=0.10)

train_masks = {}
train_imgs = {}
for id_ in tqdm(imageid):
    img = cv2.resize(load_image_disk(id_), (new_shape[0], new_shape[1]))
    
    if len(img.shape) == 2:
        img = np.expand_dims(img, axis=2)
    train_imgs[id_] = img
    train_masks[id_] = np.expand_dims(cv2.resize(load_mask_disk(id_), (new_shape[0], new_shape[1])), axis=2)
    
    
def randomShiftScaleRotate(image, mask,
                           shift_limit=(-0.0625, 0.0625),
                           scale_limit=(-0.1, 0.1),
                           rotate_limit=(-45, 45), aspect_limit=(0, 0),
                           borderMode=cv2.BORDER_REFLECT_101, u=0.5):
    if np.random.random() < u:
        height, width, channels = image.shape

        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])  # degree
        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])
        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])
        sx = scale * aspect / (aspect ** 0.5)
        sy = scale / (aspect ** 0.5)
        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)
        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)

        cc = np.math.cos(angle / 180 * np.math.pi) * sx
        ss = np.math.sin(angle / 180 * np.math.pi) * sy
        rotate_matrix = np.array([[cc, -ss], [ss, cc]])

        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])
        box1 = box0 - np.array([width / 2, height / 2])
        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])

        box0 = box0.astype(np.float32)
        box1 = box1.astype(np.float32)
        mat = cv2.getPerspectiveTransform(box0, box1)
        
        
        
        
        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,
                                    borderValue=(0, 0, 0,))
        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,
                                   borderValue=(0, 0, 0,))
        
        
        
        if len(mask.shape) == 2:
            mask = np.expand_dims(mask, axis=2)
            
            

    return image, mask

def randomHorizontalFlip(image, mask, u=0.5):
    if np.random.random() < u:
        image = cv2.flip(image, 1)
        mask = cv2.flip(mask, 1)

    return image, mask


class DataGenerator(Sequence):
    'Generates data for Keras'
    def __init__(self, list_IDs, batch_size=2, dim=(HEIGHT, WIDTH), n_channels=1,
                 n_classes=2, shuffle=True, augment=True):
        
        'list_IDs is a list of the images in the training set'
        self.dim = dim
        self.batch_size = batch_size
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.augment = augment
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        X_batch = []
        Y_batch = []
        
        for i, img_id in enumerate(list_IDs_temp):
            
            x = get_image(img_id)
            y = get_mask(img_id)
            
            if self.augment == True:

                x, y = randomShiftScaleRotate(x, y,
                                              shift_limit=(-0.0625, 0.0625),
                                              scale_limit=(-0.1, 0.1),
                                              rotate_limit=(-45, 45))

                x, y = randomHorizontalFlip(x, y)
                
                x=x.astype(np.float32)
                x=(x-np.mean(x))/np.std(x)
                y=y.astype(np.float32)
                y = np.where(y<0.5,0,1)
                
                
            if self.augment == False:
                
                #x=x.astype(np.float32)/255
                x=x.astype(np.float32)
                x=(x-np.mean(x))/np.std(x)
                y=y.astype(np.float32)
                y = np.where(y<0.5,0,1)                
                
            x=x.reshape([HEIGHT, WIDTH, CHANNELS])
            y=y.reshape([HEIGHT, WIDTH, CHANNELS])

            X_batch.append(x)
            Y_batch.append(y)
            
        X = np.asarray(X_batch, dtype=np.float32)
        Y = np.asarray(Y_batch, dtype=np.float32)
        
        return X, Y


def get_model_memory_usage(batch_size, model):
    from keras import backend as K

    shapes_mem_count = 0
    for l in model.layers:
        single_layer_mem = 1
        for s in l.output_shape:
            if s is None:
                continue
            single_layer_mem *= s
        shapes_mem_count += single_layer_mem

    trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))
    non_trainable_count = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))

    total_memory = 4*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)
    gbytes = round(total_memory / (1024 ** 3), 3)
    mbytes = round(total_memory / (1024 ** 2), 3)
    
    print('trainable_count', trainable_count, 'non_trainable_count', non_trainable_count, 'gbytes', gbytes, 'mbytes', mbytes)
    

def down_1(filters, input_):
    res_path = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(input_)
    res_path = BatchNormalization()(res_path)
    res_path = Activation(activation='relu')(res_path)
    res_path = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(res_path)
    
    shortcut = Conv2D(filters, kernel_size=(1, 1))(input_)
    shortcut = BatchNormalization()(shortcut)
    
    down_res = add([shortcut, res_path])
    #down_res = Activation('relu')(res_path)
    
    down_pool = MaxPooling2D((2, 2), strides=(2, 2))(down_res)
    return down_pool, down_res


def down(filters, input_):
    
    res_path = BatchNormalization()(input_)
    res_path = Activation(activation='relu')(res_path)
    res_path = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(res_path)
    res_path = BatchNormalization()(res_path)
    res_path = Activation(activation='relu')(res_path)
    res_path = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(res_path)
    
    shortcut = Conv2D(filters, kernel_size=(1, 1))(input_)
    shortcut = BatchNormalization()(shortcut)
    
    down_res = add([shortcut, res_path])
    down_pool = MaxPooling2D((2, 2), strides=(2, 2))(down_res)
    
    return down_pool, down_res

def up(filters, input_, down_):
    up_ = UpSampling2D((2, 2))(input_)
    up_ = concatenate([down_, up_], axis=3)
    
    res_path = BatchNormalization()(up_)
    res_path = Activation(activation='relu')(res_path)
    res_path = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(res_path)
    res_path = BatchNormalization()(res_path)
    res_path = Activation(activation='relu')(res_path)
    res_path = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(res_path)
    
    shortcut = Conv2D(filters, kernel_size=(1, 1))(up_)
    shortcut = BatchNormalization()(shortcut)
    
    up_ = add([shortcut, res_path])

    return up_

def get_unet_1024(input_shape=(HEIGHT, WIDTH, CHANNELS), num_classes=1):


    with tf.device('/gpu:0'):
        inputs = Input(shape=input_shape)
        #down0b, down0b_res = down(8, inputs)
        down0a, down0a_res = down_1(24, inputs)
        down0, down0_res = down(64, down0a)
        down1, down1_res = down(128, down0)
        down2, down2_res = down(256, down1)
        down3, down3_res = down(512, down2)
        down4, down4_res = down(1024, down3)
        
        center = BatchNormalization(epsilon=1e-4)(down4)
        center = Activation('relu')(center)
        center = Conv2D(1024, (3, 3), padding='same')(center)
        center = BatchNormalization(epsilon=1e-4)(center)
        center = Activation('relu')(center)
        center = Conv2D(1024, (3, 3), padding='same')(center)
        
        shortcut = Conv2D(1024, kernel_size=(1, 1))(down4)
        shortcut = BatchNormalization()(shortcut)

        center = add([shortcut, center])
    
        up4 = up(1024, center, down4_res)
        up3 = up(512, up4, down3_res)
        up2 = up(256, up3, down2_res)
        up1 = up(128, up2, down1_res)
        up0 = up(64, up1, down0_res)
        up0a = up(24, up0, down0a_res)
        #up0b = up(8, up0a, down0b_res)
    
        classify = Conv2D(num_classes, (1, 1), activation='sigmoid', name='final_layer')(up0a)
    
        model = Model(inputs=inputs, outputs=classify)

    return model



def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_coef_loss(y_true, y_pred):
    return 1-dice_coef(y_true, y_pred)

def bce_dice_loss(y_true, y_pred):
    return binary_crossentropy(y_true, y_pred) + dice_coef_loss(y_true, y_pred)





# Training new model
ts = str(int(time.time()))
model_name = 'u_net_nucleus'
num_epochs = 30
steps_per_epoch = int(len(imageid) /BATCH_SIZE)
run_name = 'model={}-batch_size={}-num_epoch={}-steps_per_epoch={}-ts={}'.format(model_name,
                                                                          BATCH_SIZE,
                                                                          num_epochs,
                                                                          steps_per_epoch,
                                                                          ts)
tensorboard_loc = os.path.join(TENSORBOARD_PATH, run_name)
checkpoint_loc = os.path.join(MODELS_PATH, 'model-{}-weights.h5'.format(ts))

earlyStopping = EarlyStopping(monitor='val_loss', 
                              patience=4, 
                              verbose=1, 
                              min_delta = 0.0001,
                              mode='min',)

modelCheckpoint = ModelCheckpoint(checkpoint_loc,
                                  monitor = 'val_loss', 
                                  save_best_only = True, 
                                  mode = 'min', 
                                  verbose = 1,
                                  save_weights_only = True)

tensorboard = TensorBoard(log_dir=tensorboard_loc, histogram_freq=0, write_graph=True, write_images=True)

callbacks_list = [ tensorboard]

model = get_unet_1024()
model.compile(loss=bce_dice_loss, optimizer=Adam(lr=1e-5), metrics=[dice_coef, 'acc'])
#print(model.summary())
get_model_memory_usage(BATCH_SIZE, model)




train_generator = DataGenerator(train_ids, batch_size=BATCH_SIZE, augment=True)
valid_generator = DataGenerator(validation_ids, batch_size=BATCH_SIZE, augment=False, shuffle=False)
test_generator = DataGenerator(test_ids, batch_size=BATCH_SIZE, augment=False, shuffle=False)
VALIDATION_STEPS = int(len(validation_ids) / BATCH_SIZE)



print('Starting run {}'.format(run_name))
history = model.fit_generator(
        train_generator, 
        steps_per_epoch = steps_per_epoch, 
        epochs = num_epochs,
        callbacks = callbacks_list,
        verbose = 1,
        validation_data = valid_generator,
        validation_steps = VALIDATION_STEPS,
        use_multiprocessing = True,
        workers=6,
        max_queue_size=30)

model_path = os.path.join(MODELS_PATH, 'model-{}.h5'.format(ts))
history_path = os.path.join(MODELS_PATH, 'model-{}.history'.format(ts))
model.save(model_path)
pickle.dump(history.history, open(history_path, "wb"))
print('Saved model at {}'.format(model_path))
print('Saved model history at {}'.format(history_path))



model = get_unet_1024()
model.load_weights(os.path.join(MODELS_PATH, 'model-{}.h5'.format(ts)))
model.compile(loss=bce_dice_loss, optimizer=Adam(1e-5), metrics=[dice_coef, 'acc'])


test_scores=model.evaluate_generator(test_generator)

#preds=model.evaluate_generator(test_generator)

pred_mask=model.predict_generator(test_generator)

pred_mask = np.where(pred_mask<0.5,0,1)

test_img=[get_image(i) for i in test_ids]
test_mask=[get_mask(i) for i in test_ids]


for k in range(3):
    
    i=np.random.choice(range(len(test_ids)))
    
    plt.figure()
    plt.subplot(1,3,1)
    plt.title("Prediction")
    plt.imshow(np.squeeze(pred_mask[i,:,:,:])/255)
    
    plt.subplot(1,3,2)
    plt.title("Ground Truth")
    plt.imshow((np.squeeze(test_mask[i])/255))
    
    plt.subplot(1,3,3)
    plt.title("Original Image")
    plt.imshow((np.squeeze(test_img[i])/255))
    
  
